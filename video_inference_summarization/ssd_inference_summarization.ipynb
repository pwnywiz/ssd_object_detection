{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD Video Inference and Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN, CSVLogger\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from models.keras_ssd7 import build_model\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "\n",
    "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "from data_generator.data_augmentation_chain_variable_input_size import DataAugmentationVariableInputSize\n",
    "from data_generator.data_augmentation_chain_constant_input_size import DataAugmentationConstantInputSize\n",
    "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
    "\n",
    "from io import StringIO\n",
    "from PIL import Image\n",
    "from statistics import mean\n",
    "\n",
    "import cv2\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the model configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_height = 300\n",
    "img_width = 480\n",
    "img_channels = 3\n",
    "intensity_mean = None\n",
    "intensity_range = None\n",
    "n_classes = 5\n",
    "scales = [0.08, 0.16, 0.32, 0.64, 0.96]\n",
    "aspect_ratios = [0.5, 1.0, 2.0]\n",
    "two_boxes_for_ar1 = True\n",
    "steps = None\n",
    "offsets = None\n",
    "clip_boxes = False\n",
    "variances = [1.0, 1.0, 1.0, 1.0]\n",
    "normalize_coords = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build or load the model\n",
    "\n",
    "We're loading the trained weights and using the same parameters that were used for training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = build_model(image_size=(img_height, img_width, img_channels),\n",
    "                    n_classes=n_classes,\n",
    "                    mode='inference',\n",
    "                    l2_regularization=0.0007,\n",
    "                    scales=scales,\n",
    "                    aspect_ratios_global=aspect_ratios,\n",
    "                    aspect_ratios_per_layer=None,\n",
    "                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                    steps=steps,\n",
    "                    offsets=offsets,\n",
    "                    clip_boxes=clip_boxes,\n",
    "                    variances=variances,\n",
    "                    normalize_coords=normalize_coords,\n",
    "                    subtract_mean=intensity_mean,\n",
    "                    divide_by_stddev=intensity_range)\n",
    "\n",
    "model.load_weights('C:/Users/Ignatios/Documents/ssd_keras/model_weights/custom/ssd7_epoch-19_loss-1.4077_val_loss-1.8736.h5', by_name=True)\n",
    "\n",
    "adam = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper Classes for storing and analyzing frame details from the video's inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetails:\n",
    "    def __init__(self, class_obj, xmin, ymin, xmax, ymax):\n",
    "        self.class_obj = class_obj\n",
    "        self.xmin = xmin\n",
    "        self.ymin = ymin\n",
    "        self.xmax = xmax\n",
    "        self.ymax = ymax\n",
    "\n",
    "class AverageObjectDetails(ObjectDetails):\n",
    "    def __init__(self, class_obj, visible_frames, visible_percentage, start_frame, end_frame, xmin, ymin, xmax, ymax):\n",
    "        super().__init__(class_obj, xmin, ymin, xmax, ymax)\n",
    "        self.visible_frames = visible_frames\n",
    "        self.visible_percentage = visible_percentage\n",
    "        self.start_frame = start_frame\n",
    "        self.end_frame = end_frame\n",
    "    \n",
    "    def fill_frames(self):\n",
    "        self.visible_frames = list(range(self.start_frame, self.end_frame + 1))\n",
    "        self.visible_percentage = 1.0\n",
    "\n",
    "class ObjectSummary:\n",
    "    def __init__(self, class_num, visible_frames):\n",
    "        self.class_num = class_num\n",
    "        self.visible_frames = visible_frames\n",
    "    \n",
    "    def update_frames(self, frames):\n",
    "        self.visible_frames.extend(frames)\n",
    "\n",
    "class FrameDetails:\n",
    "    def __init__(self, frame, frame_objects):\n",
    "        self.frame = frame\n",
    "        self.frame_objects = frame_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper functions for pre-processing the video inference and applying slight enhancements on some frame details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accepts a list of all frames (and their details) and the video FPS output and\n",
    "# breaks it down to half second windows (e.g. windows of 15 frames for videos at 30 FPS)\n",
    "def break_in_half_second_windows(detections_in_seconds, out_frames):\n",
    "    frame_window = int(out_frames / 2)\n",
    "    frames_length = len(detections_in_seconds)\n",
    "    frames_in_windows = []\n",
    "    for i in range(0, frames_length, frame_window):\n",
    "        frames_in_windows.append(detections_in_seconds[i:i + frame_window])\n",
    "    \n",
    "    return frames_in_windows\n",
    "\n",
    "# Accepts two bounding boxes and calculates\n",
    "# the Intersection Over Union (IOU) percentage\n",
    "def intersection_over_union(boxA, boxB):\n",
    "\txA = max(boxA.xmin, boxB.xmin)\n",
    "\tyA = max(boxA.ymin, boxB.ymin)\n",
    "\txB = min(boxA.xmax, boxB.xmax)\n",
    "\tyB = min(boxA.ymax, boxB.ymax)\n",
    "\n",
    "\tinterArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\n",
    "\tboxAArea = (boxA.xmax - boxA.xmin + 1) * (boxA.ymax - boxA.ymin + 1)\n",
    "\tboxBArea = (boxB.xmax - boxB.xmin + 1) * (boxB.ymax - boxB.ymin + 1)\n",
    "\n",
    "\tiou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "\treturn iou\n",
    "\n",
    "# Analyzes the window of frames by gathering info\n",
    "# regarding the average location of bounding boxes,\n",
    "# the frames that each class is found at and the\n",
    "# average percentage of participation\n",
    "def analyze_window(window):\n",
    "    window_details = {}\n",
    "    object_intersections = {}\n",
    "    start_frame = window[0].frame\n",
    "    end_frame = window[-1].frame\n",
    "    for frame_details in window:\n",
    "        for frame_obj in frame_details.frame_objects:\n",
    "            class_num = frame_obj.class_obj\n",
    "            if class_num in window_details.keys():\n",
    "                window_details[class_num][0].append(frame_obj.xmin)\n",
    "                window_details[class_num][1].append(frame_obj.ymin)\n",
    "                window_details[class_num][2].append(frame_obj.xmax)\n",
    "                window_details[class_num][3].append(frame_obj.ymax)\n",
    "                window_details[class_num][4].append(frame_details.frame)\n",
    "            else:\n",
    "                window_details[class_num] = []\n",
    "                window_details[class_num].append([frame_obj.xmin])\n",
    "                window_details[class_num].append([frame_obj.ymin])\n",
    "                window_details[class_num].append([frame_obj.xmax])\n",
    "                window_details[class_num].append([frame_obj.ymax])\n",
    "                window_details[class_num].append([frame_details.frame])\n",
    "    \n",
    "    # Gather the average bounding box location and total frames\n",
    "    # individually for each class found inside this window\n",
    "    analyzed_classes = []\n",
    "    for class_num in window_details.keys():\n",
    "        xmin_average = mean(window_details[class_num][0])\n",
    "        ymin_average = mean(window_details[class_num][1])\n",
    "        xmax_average = mean(window_details[class_num][2])\n",
    "        ymax_average = mean(window_details[class_num][3])\n",
    "        visible_frames = window_details[class_num][4]\n",
    "        visible_percentage = round(len(visible_frames)/len(window), 3)\n",
    "        analyzed_classes.append(AverageObjectDetails(class_num, visible_frames, visible_percentage, start_frame, end_frame, xmin_average, ymin_average, xmax_average, ymax_average))\n",
    "    \n",
    "    return analyzed_classes\n",
    "\n",
    "# Applies some enhancements on a window by deciding\n",
    "# if the objects appearing are actually relevant or\n",
    "# possible misclassifications\n",
    "def optimize_window(analyzed_classes):\n",
    "    analyzed_classes = list(filter(lambda ac: ac.visible_percentage >= 0.2, analyzed_classes))\n",
    "    for analyzed_class in analyzed_classes:\n",
    "        candidate_classes = list(filter(lambda c: c.class_obj != analyzed_class.class_obj, analyzed_classes))\n",
    "        if len(analyzed_classes) > 1:\n",
    "            for candidate in candidate_classes:\n",
    "                if intersection_over_union(analyzed_class, candidate) > 0.5 and candidate.visible_percentage > analyzed_class.visible_percentage:\n",
    "                    analyzed_classes = candidate_classes\n",
    "                    break\n",
    "        if analyzed_class.visible_percentage > 0.7 and analyzed_class.visible_percentage < 1.0:\n",
    "            analyzed_class.fill_frames()\n",
    "            \n",
    "    return analyzed_classes\n",
    "\n",
    "# Accepts the raw detections from video inference and applies transformations\n",
    "# by breaking them down to windows, enhancing frames or removing objects that\n",
    "# have been potentially misclassified by the predictor\n",
    "def windowing_frame_fixing(detections_in_seconds, out_frames, classes):\n",
    "    class_dict = {}\n",
    "    for class_num in classes:\n",
    "        class_dict[class_num] = ObjectSummary(class_num, [])\n",
    "\n",
    "    half_second_windows = break_in_half_second_windows(detections_in_seconds, out_frames)\n",
    "\n",
    "    for window in half_second_windows:\n",
    "        analyzed_classes = analyze_window(window)\n",
    "        filtered_classes = optimize_window(analyzed_classes)\n",
    "\n",
    "        for filtered_class in filtered_classes:\n",
    "            class_dict[filtered_class.class_obj].update_frames(filtered_class.visible_frames)\n",
    "    \n",
    "    return class_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper functions for extracting summary details for each class of interest, as well as co-appearance statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the intersection of frames\n",
    "def co_appearance(lst1, lst2): \n",
    "    lst_to_set = frozenset(lst2)\n",
    "    return [x for x in lst1 if x in lst_to_set]\n",
    "\n",
    "# Accepts an array of all the frames where an object appears and\n",
    "# the framerate of the video. It breaks it down to non - continuous\n",
    "# parts and then converts them to distinct arrays of start - end\n",
    "# pairs in seconds\n",
    "def frames_to_distinct_scenes(class_frames, out_frames):\n",
    "    # return if the object has not been detected\n",
    "    if not class_frames: return []\n",
    "\n",
    "    # break down frames into non - continuous scenes\n",
    "    distinct_scenes = [[class_frames[0]]]\n",
    "    for i in range(len(class_frames)):\n",
    "        if i > 0:\n",
    "            last_frame = distinct_scenes[-1][-1]\n",
    "            frame_diff = class_frames[i] - last_frame\n",
    "            if (frame_diff > 1):\n",
    "                distinct_scenes.append([class_frames[i]])\n",
    "            else:\n",
    "                distinct_scenes[-1].append(class_frames[i])\n",
    "\n",
    "    # keep the first and last frame of each scene, converted in seconds\n",
    "    scenes_in_seconds = []\n",
    "    for scene in distinct_scenes:\n",
    "        start_in_seconds = round(scene[0] / out_frames, 2)\n",
    "        end_in_seconds = round(scene[-1] / out_frames, 2)\n",
    "        scenes_in_seconds.append([start_in_seconds, end_in_seconds])\n",
    "    \n",
    "    return scenes_in_seconds\n",
    "\n",
    "# Accepts a class of interest, an array with details for every frame of the clip\n",
    "# and the FPS of the video. Extracts analytical details for the class of interest\n",
    "# and the classes that appeared with it at the same frames\n",
    "def extract_summary_for_class(class_num, optimized_frames, out_frames):\n",
    "    class_of_interest = optimized_frames[class_num]\n",
    "    scenes_in_seconds = frames_to_distinct_scenes(class_of_interest.visible_frames, out_frames)\n",
    "\n",
    "    co_appearance_frames = {}\n",
    "    for class_key in optimized_frames.keys():\n",
    "        if class_key != class_num:\n",
    "            same_frames = co_appearance(class_of_interest.visible_frames, optimized_frames[class_key].visible_frames)\n",
    "            co_appearance_frames[class_key] = same_frames\n",
    "    \n",
    "    return len(class_of_interest.visible_frames), scenes_in_seconds, co_appearance_frames\n",
    "\n",
    "# Accepts a dictionary of classes that co-appeared with a particular class\n",
    "# and extracts an analytical summary of time and duration, relative to the\n",
    "# framerate of the video of interest\n",
    "def extract_co_appearance_stats(co_appearance_frames, out_frames):\n",
    "    stats_per_class = {}\n",
    "\n",
    "    for co_appearance in co_appearance_frames:\n",
    "        class_frames = co_appearance_frames[co_appearance]\n",
    "        scenes_in_seconds = frames_to_distinct_scenes(class_frames, out_frames)\n",
    "        stats_per_class.update({co_appearance: scenes_in_seconds})\n",
    "    \n",
    "    return stats_per_class\n",
    "\n",
    "# Accept a time step in seconds and converts\n",
    "# it to minutes, seconds, miliseconds format\n",
    "def format_time(seconds): \n",
    "    ms, sec = np.modf(seconds)\n",
    "    ms = (round(ms, 2)) * 100\n",
    "    min, sec = divmod(sec, 60) \n",
    "    return \"%01d:%02d.%02d\" % (min, sec, ms)\n",
    "\n",
    "# Prints a time frame in a formatted way\n",
    "def print_time_frame(distinct_scene):\n",
    "    if round(distinct_scene[1] - distinct_scene[0], 2) > 0.0:\n",
    "        print('At {} for {} seconds'.format(format_time(distinct_scene[0]), round(distinct_scene[1] - distinct_scene[0], 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper functions for processing the video, making predictions using the model and drawing bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_TYPE = {\n",
    "    'avi': cv2.VideoWriter_fourcc(*'XVID'),\n",
    "    'mp4': cv2.VideoWriter_fourcc(*'XVID'),\n",
    "}\n",
    "\n",
    "# Cleans multiple bounding boxes for each class by selecting the one with\n",
    "# the highest confidence, since we care about unique classes in each frame\n",
    "def clean_multiple_confidence_boxes(positive_classes, y_pred_thresh):\n",
    "    filtered_confidence_boxes = []\n",
    "    for positive_class in positive_classes:\n",
    "        class_boxes = list(filter(lambda c: c[0] == positive_class, y_pred_thresh))\n",
    "        if class_boxes:\n",
    "            filtered_confidence_boxes.append(max(class_boxes, key = lambda c: c[1]))\n",
    "    return filtered_confidence_boxes\n",
    "\n",
    "def get_video_type(filename):\n",
    "    filename, ext = os.path.splitext(filename)\n",
    "    if ext in VIDEO_TYPE:\n",
    "      return  VIDEO_TYPE[ext]\n",
    "    return VIDEO_TYPE['avi']\n",
    "\n",
    "# Applies the model on every frame of the video and\n",
    "# draws bounding boxes over the detected objects as\n",
    "# well as the FPS at which the video is processed\n",
    "def video_inference():\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get original video width and height\n",
    "    orig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    orig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Create a VideoWriter object to save the output video\n",
    "    out = cv2.VideoWriter(save_path, get_video_type(save_path), out_frames, (orig_width, orig_height))\n",
    "\n",
    "    all_frames_objects = []\n",
    "    accum_time = 0\n",
    "    curr_fps = 0\n",
    "    curr_frame = 0\n",
    "    fps = \"FPS: ??\"\n",
    "    prev_time = timer()\n",
    "\n",
    "    # Open the video and read frame by frame\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if ret == False:\n",
    "            break\n",
    "\n",
    "        # Convert the frame to RGB format\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Resize the frame to model input acceptable dimensions\n",
    "        input_images = []\n",
    "        img = cv2.resize(rgb_frame, (img_width, img_height))\n",
    "        input_images.append(img)\n",
    "        input_images = np.array(input_images)\n",
    "\n",
    "        y_pred = model.predict(input_images)\n",
    "\n",
    "        # Get aspect ratio between original and model input dimensions\n",
    "        width_aspect = frame.shape[1] / img_width\n",
    "        height_aspect = frame.shape[0] / img_height\n",
    "\n",
    "        # Filter out the predictions that are below the defined confidence threshold\n",
    "        y_pred_thresh = [y_pred[k][y_pred[k,:,1] > confidence_threshold] for k in range(y_pred.shape[0])]\n",
    "\n",
    "        # Filter out multiple bounding boxes with lower confidence for the same class\n",
    "        y_pred_thresh = clean_multiple_confidence_boxes(numerical_positive_classes, y_pred_thresh[0])\n",
    "\n",
    "        frame_objects = []\n",
    "\n",
    "        # Calculate and draw the bounding boxes for each predicted class\n",
    "        for box in y_pred_thresh:\n",
    "            xmin = int(round(box[2] * width_aspect))\n",
    "            ymin = int(round(box[3] * height_aspect))\n",
    "            xmax = int(round(box[4] * width_aspect))\n",
    "            ymax = int(round(box[5] * height_aspect))\n",
    "            color = colors[int(box[0])]\n",
    "            label = '{}: {:.2f}'.format(classes[int(box[0])], box[1])\n",
    "\n",
    "            frame_objects.append(ObjectDetails(int(box[0]), xmin, ymin, xmax, ymax))\n",
    "\n",
    "            tl=(xmin, ymin)\n",
    "            br=(xmax, ymax)\n",
    "\n",
    "            cv2.rectangle(frame, tl, br, color, 2)\n",
    "            \n",
    "            text_top = (xmin, ymin-10)\n",
    "            text_bot = (xmin + 80, ymin + 5)\n",
    "            text_pos = (xmin + 5, ymin)\n",
    "            cv2.rectangle(frame, text_top, text_bot, color, -1)\n",
    "            cv2.putText(frame, label, text_pos, cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)\n",
    "        \n",
    "        # Add the prediction details for summarization purposes\n",
    "        all_frames_objects.append(FrameDetails(curr_frame, frame_objects))\n",
    "        curr_frame = curr_frame + 1\n",
    "\n",
    "        # Calculate FPS\n",
    "        curr_time = timer()\n",
    "        exec_time = curr_time - prev_time\n",
    "        prev_time = curr_time\n",
    "        accum_time = accum_time + exec_time\n",
    "        curr_fps = curr_fps + 1\n",
    "        \n",
    "        if accum_time > 1:\n",
    "            accum_time = accum_time - 1\n",
    "            fps = \"FPS: \" + str(curr_fps)\n",
    "            curr_fps = 0\n",
    "        \n",
    "        # Draw FPS on the top left corner\n",
    "        cv2.rectangle(frame, (0,0), (50, 17), (255,255,255), -1)\n",
    "        cv2.putText(frame, fps, (3,10), cv2.FONT_HERSHEY_SIMPLEX, 0.35, (0,0,0), 1)\n",
    "\n",
    "        cv2.imshow('frame', frame)\n",
    "        out.write(frame)\n",
    "\n",
    "        # Access frames on max speed and wait for closing signal\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    out.release()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return all_frames_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Inference\n",
    "\n",
    "- Select the input video and output video save location / framerate\n",
    "- Set a confidence threshold for bounding box inference\n",
    "- Set the desired bounding box colour to be drawn for each class\n",
    "- Set the classes to be predicted from the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_frames = 30\n",
    "save_path = 'C:/Users/Ignatios/Downloads/videoplayback_out.avi'\n",
    "video_path = 'C:/Users/Ignatios/Downloads/final_video.mp4'\n",
    "\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "colors = [(0,0,0), (32,178,170), (34,139,34), (34,139,34), (34,139,34), (34,139,34)]\n",
    "\n",
    "classes = ['background', 'BB8', 'R2D2', 'Yoda', 'Chewbacca', 'Stormtrooper']\n",
    "numerical_positive_classes = [1, 2, 3, 4, 5]\n",
    "\n",
    "all_frames_detections = video_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Summary\n",
    "\n",
    "- Pre-process the collected frames by applying enhancements and filtering possible misclassifications\n",
    "- Iterate over all classes of interest\n",
    "- Extract video summary for every class\n",
    "- Extract co-appearance summary for every class\n",
    "- Print summary details in a formatted way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total screen time for BB8 is 39.7 seconds\n\nSpecifically, BB8 appeared:\nAt 0:11.40 for 1.73 seconds\nAt 0:14.50 for 0.47 seconds\nAt 0:17.77 for 1.4 seconds\nAt 0:22.50 for 0.8 seconds\nAt 0:24.87 for 8.43 seconds\nAt 0:39.70 for 1.77 seconds\nAt 0:41.77 for 0.06 seconds\nAt 0:42.00 for 1.77 seconds\nAt 0:44.33 for 1.64 seconds\nAt 0:46.67 for 5.36 seconds\nAt 0:52.37 for 0.16 seconds\nAt 0:52.73 for 1.74 seconds\nAt 0:54.73 for 0.9 seconds\nAt 0:56.40 for 0.57 seconds\nAt 0:57.20 for 0.13 seconds\nAt 0:57.50 for 0.57 seconds\nAt 0:58.33 for 3.84 seconds\nAt 1:03.90 for 1.07 seconds\nAt 1:05.50 for 2.47 seconds\nAt 1:09.20 for 1.87 seconds\nAt 1:12.00 for 1.07 seconds\nAt 1:14.87 for 1.1 seconds\n\nBB8 co-appeared with R2D2\nAt 0:24.87 for 3.86 seconds\n\nBB8 co-appeared with Yoda\nAt 1:00.37 for 0.1 seconds\n\nBB8 co-appeared with Chewbacca\nAt 0:42.50 for 0.13 seconds\n\n\n\nTotal screen time for R2D2 is 13.07 seconds\n\nSpecifically, R2D2 appeared:\nAt 0:03.00 for 2.7 seconds\nAt 0:16.17 for 1.56 seconds\nAt 0:23.33 for 5.4 seconds\nAt 2:46.50 for 2.23 seconds\nAt 3:13.00 for 0.03 seconds\nAt 3:52.87 for 0.23 seconds\nAt 3:53.47 for 0.5 seconds\nAt 3:55.40 for 0.07 seconds\n\nR2D2 co-appeared with BB8\nAt 0:24.87 for 3.86 seconds\n\nR2D2 co-appeared with Stormtrooper\nAt 3:52.87 for 0.23 seconds\nAt 3:53.47 for 0.5 seconds\nAt 3:55.40 for 0.07 seconds\n\n\n\nTotal screen time for Yoda is 59.27 seconds\n\nSpecifically, Yoda appeared:\nAt 0:33.80 for 0.23 seconds\nAt 0:34.33 for 0.64 seconds\nAt 0:39.60 for 0.07 seconds\nAt 1:00.37 for 0.1 seconds\nAt 1:11.53 for 0.17 seconds\nAt 1:16.00 for 1.63 seconds\nAt 1:18.00 for 8.63 seconds\nAt 1:26.97 for 0.5 seconds\nAt 1:27.56 for 0.06 seconds\nAt 1:28.00 for 0.47 seconds\nAt 1:31.27 for 1.7 seconds\nAt 1:36.50 for 0.47 seconds\nAt 2:19.50 for 0.03 seconds\nAt 2:19.70 for 0.03 seconds\nAt 2:21.20 for 0.37 seconds\nAt 2:32.00 for 0.47 seconds\nAt 2:57.40 for 0.07 seconds\nAt 2:58.40 for 0.07 seconds\nAt 2:59.33 for 0.2 seconds\nAt 2:59.60 for 0.1 seconds\nAt 4:07.00 for 8.73 seconds\nAt 4:23.00 for 3.77 seconds\nAt 4:27.10 for 0.27 seconds\nAt 4:28.13 for 0.24 seconds\nAt 4:28.47 for 0.03 seconds\nAt 4:28.87 for 3.83 seconds\nAt 4:37.30 for 12.27 seconds\nAt 4:53.87 for 3.1 seconds\nAt 4:58.77 for 2.5 seconds\nAt 5:04.50 for 7.3 seconds\n\nYoda co-appeared with BB8\nAt 1:00.37 for 0.1 seconds\n\n\n\nTotal screen time for Chewbacca is 45.13 seconds\n\nSpecifically, Chewbacca appeared:\nAt 0:42.50 for 0.13 seconds\nAt 0:52.56 for 0.13 seconds\nAt 1:33.00 for 2.47 seconds\nAt 1:35.77 for 0.7 seconds\nAt 1:37.17 for 0.6 seconds\nAt 1:37.97 for 0.5 seconds\nAt 1:42.33 for 7.14 seconds\nAt 2:00.17 for 0.1 seconds\nAt 2:15.80 for 0.1 seconds\nAt 2:16.27 for 0.03 seconds\nAt 2:16.47 for 1.03 seconds\nAt 2:17.60 for 0.07 seconds\nAt 2:17.87 for 0.66 seconds\nAt 2:18.60 for 0.03 seconds\nAt 2:18.80 for 0.23 seconds\nAt 2:19.13 for 0.04 seconds\nAt 2:19.33 for 0.14 seconds\nAt 2:20.30 for 0.17 seconds\nAt 2:43.07 for 0.23 seconds\nAt 2:44.30 for 0.33 seconds\nAt 2:48.77 for 2.36 seconds\nAt 2:55.60 for 0.1 seconds\nAt 2:56.20 for 0.07 seconds\nAt 3:04.00 for 1.1 seconds\nAt 3:08.17 for 0.1 seconds\nAt 3:08.37 for 0.06 seconds\nAt 3:10.30 for 2.47 seconds\nAt 3:18.67 for 2.6 seconds\nAt 3:22.17 for 1.8 seconds\nAt 3:24.50 for 2.07 seconds\nAt 3:27.70 for 3.33 seconds\nAt 3:31.20 for 0.1 seconds\nAt 3:31.37 for 1.1 seconds\nAt 3:33.17 for 0.1 seconds\nAt 3:33.33 for 0.84 seconds\nAt 3:34.70 for 0.07 seconds\nAt 3:34.90 for 0.07 seconds\nAt 3:35.20 for 0.07 seconds\nAt 3:35.37 for 0.06 seconds\nAt 3:35.73 for 1.4 seconds\nAt 3:38.77 for 0.06 seconds\nAt 3:52.53 for 0.14 seconds\nAt 4:01.87 for 0.06 seconds\nAt 4:02.00 for 0.03 seconds\nAt 4:02.27 for 0.26 seconds\nAt 4:03.67 for 3.3 seconds\nAt 4:32.73 for 4.54 seconds\nAt 5:03.70 for 0.27 seconds\n\nChewbacca co-appeared with BB8\nAt 0:42.50 for 0.13 seconds\n\nChewbacca co-appeared with Stormtrooper\nAt 3:04.00 for 1.1 seconds\nAt 3:10.30 for 2.47 seconds\nAt 3:18.67 for 0.8 seconds\nAt 3:19.60 for 0.03 seconds\nAt 3:19.77 for 0.06 seconds\nAt 3:21.00 for 0.27 seconds\nAt 3:22.77 for 0.73 seconds\nAt 3:23.70 for 0.07 seconds\nAt 3:23.93 for 0.04 seconds\nAt 3:24.50 for 2.07 seconds\nAt 3:27.93 for 2.7 seconds\nAt 3:30.93 for 0.1 seconds\nAt 3:31.20 for 0.1 seconds\nAt 3:31.37 for 1.1 seconds\nAt 3:33.17 for 0.1 seconds\nAt 3:33.33 for 0.14 seconds\nAt 3:33.77 for 0.4 seconds\nAt 3:35.37 for 0.06 seconds\nAt 3:35.73 for 1.4 seconds\nAt 3:38.77 for 0.06 seconds\nAt 3:52.53 for 0.14 seconds\nAt 4:01.87 for 0.06 seconds\nAt 4:02.00 for 0.03 seconds\nAt 4:02.27 for 0.26 seconds\nAt 4:03.67 for 3.3 seconds\n\n\n\nTotal screen time for Stormtrooper is 66.2 seconds\n\nSpecifically, Stormtrooper appeared:\nAt 1:02.20 for 0.77 seconds\nAt 1:05.00 for 0.47 seconds\nAt 1:55.30 for 1.83 seconds\nAt 2:00.50 for 1.47 seconds\nAt 2:02.50 for 0.97 seconds\nAt 2:04.17 for 0.06 seconds\nAt 2:07.33 for 0.04 seconds\nAt 2:07.47 for 0.6 seconds\nAt 2:08.13 for 0.04 seconds\nAt 2:08.27 for 0.06 seconds\nAt 2:08.50 for 0.6 seconds\nAt 2:11.33 for 0.1 seconds\nAt 2:11.90 for 0.07 seconds\nAt 2:13.13 for 0.1 seconds\nAt 2:13.33 for 0.1 seconds\nAt 2:13.80 for 0.1 seconds\nAt 2:14.13 for 0.3 seconds\nAt 2:14.50 for 1.27 seconds\nAt 2:21.80 for 0.3 seconds\nAt 2:27.00 for 0.47 seconds\nAt 2:27.60 for 0.1 seconds\nAt 2:27.80 for 1.83 seconds\nAt 2:33.30 for 0.43 seconds\nAt 2:36.00 for 4.07 seconds\nAt 2:40.30 for 0.23 seconds\nAt 2:40.73 for 0.1 seconds\nAt 2:51.27 for 1.96 seconds\nAt 2:56.50 for 0.07 seconds\nAt 3:01.30 for 1.37 seconds\nAt 3:04.00 for 1.1 seconds\nAt 3:10.30 for 2.47 seconds\nAt 3:15.50 for 3.97 seconds\nAt 3:19.60 for 0.03 seconds\nAt 3:19.77 for 0.06 seconds\nAt 3:21.00 for 1.1 seconds\nAt 3:22.77 for 0.73 seconds\nAt 3:23.70 for 0.07 seconds\nAt 3:23.93 for 3.2 seconds\nAt 3:27.53 for 0.17 seconds\nAt 3:27.93 for 2.7 seconds\nAt 3:30.93 for 2.54 seconds\nAt 3:33.77 for 0.93 seconds\nAt 3:35.30 for 4.23 seconds\nAt 3:39.60 for 0.03 seconds\nAt 3:40.17 for 0.56 seconds\nAt 3:41.37 for 0.06 seconds\nAt 3:41.50 for 3.47 seconds\nAt 3:48.73 for 10.77 seconds\nAt 3:59.70 for 0.07 seconds\nAt 4:00.90 for 6.07 seconds\nAt 4:52.00 for 0.23 seconds\n\nStormtrooper co-appeared with R2D2\nAt 3:52.87 for 0.23 seconds\nAt 3:53.47 for 0.5 seconds\nAt 3:55.40 for 0.07 seconds\n\nStormtrooper co-appeared with Chewbacca\nAt 3:04.00 for 1.1 seconds\nAt 3:10.30 for 2.47 seconds\nAt 3:18.67 for 0.8 seconds\nAt 3:19.60 for 0.03 seconds\nAt 3:19.77 for 0.06 seconds\nAt 3:21.00 for 0.27 seconds\nAt 3:22.77 for 0.73 seconds\nAt 3:23.70 for 0.07 seconds\nAt 3:23.93 for 0.04 seconds\nAt 3:24.50 for 2.07 seconds\nAt 3:27.93 for 2.7 seconds\nAt 3:30.93 for 0.1 seconds\nAt 3:31.20 for 0.1 seconds\nAt 3:31.37 for 1.1 seconds\nAt 3:33.17 for 0.1 seconds\nAt 3:33.33 for 0.14 seconds\nAt 3:33.77 for 0.4 seconds\nAt 3:35.37 for 0.06 seconds\nAt 3:35.73 for 1.4 seconds\nAt 3:38.77 for 0.06 seconds\nAt 3:52.53 for 0.14 seconds\nAt 4:01.87 for 0.06 seconds\nAt 4:02.00 for 0.03 seconds\nAt 4:02.27 for 0.26 seconds\nAt 4:03.67 for 3.3 seconds\n\n\n\n"
    }
   ],
   "source": [
    "classes=['BB8', 'R2D2', 'Yoda', 'Chewbacca', 'Stormtrooper']\n",
    "\n",
    "optimized_frames = windowing_frame_fixing(all_frames_detections, out_frames, numerical_positive_classes)\n",
    "\n",
    "for selected_class, name in enumerate(classes):\n",
    "    total_frames, scenes_in_seconds, co_appearance_frames = extract_summary_for_class(selected_class + 1, optimized_frames, out_frames)\n",
    "    extract_summary_for_class(selected_class + 1, optimized_frames, out_frames)\n",
    "    print('Total screen time for {} is {} seconds'.format(classes[selected_class], round(total_frames/out_frames, 2)))\n",
    "    print('\\nSpecifically, {} appeared:'.format(classes[selected_class]))\n",
    "    for distinct_scene in scenes_in_seconds:\n",
    "         print_time_frame(distinct_scene)\n",
    "\n",
    "    stats_per_class = extract_co_appearance_stats(co_appearance_frames, out_frames)\n",
    "    for co_appeared_object in stats_per_class:\n",
    "        if len(stats_per_class[co_appeared_object]):\n",
    "            print('\\n{} co-appeared with {}'.format(classes[selected_class], classes[co_appeared_object - 1]))\n",
    "            for distinct_scene in stats_per_class[co_appeared_object]:\n",
    "                print_time_frame(distinct_scene)\n",
    "    print('\\n\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('tensorflow_gpu_object_detection': conda)",
   "language": "python",
   "name": "python37764bittensorflowgpuobjectdetectioncondabde55a80a8ba450d843c1fd84786c886"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}