{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD7 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN, CSVLogger\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from models.keras_ssd7 import build_model\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "\n",
    "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "from data_generator.data_augmentation_chain_variable_input_size import DataAugmentationVariableInputSize\n",
    "from data_generator.data_augmentation_chain_constant_input_size import DataAugmentationConstantInputSize\n",
    "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the model configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_height = 300\n",
    "img_width = 480\n",
    "img_channels = 3\n",
    "intensity_mean = None\n",
    "intensity_range = None\n",
    "n_classes = 5\n",
    "scales = [0.08, 0.16, 0.32, 0.64, 0.96]\n",
    "aspect_ratios = [0.5, 1.0, 2.0]\n",
    "two_boxes_for_ar1 = True\n",
    "steps = None\n",
    "offsets = None\n",
    "clip_boxes = False\n",
    "variances = [1.0, 1.0, 1.0, 1.0]\n",
    "normalize_coords = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow_gpu_object_detection\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:83: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n\nWARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow_gpu_object_detection\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:86: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nWARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow_gpu_object_detection\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:88: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nWARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow_gpu_object_detection\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3677: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n\nWARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow_gpu_object_detection\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1242: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\nWARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow_gpu_object_detection\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3458: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n\nWARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow_gpu_object_detection\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1208: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\nWARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow_gpu_object_detection\\lib\\site-packages\\keras\\optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\nWARNING:tensorflow:From c:\\Users\\Ignatios\\Documents\\ssd_keras\\keras_loss_function\\keras_ssd_loss.py:95: The name tf.log is deprecated. Please use tf.math.log instead.\n\nWARNING:tensorflow:From c:\\Users\\Ignatios\\Documents\\ssd_keras\\keras_loss_function\\keras_ssd_loss.py:133: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.cast` instead.\nWARNING:tensorflow:From c:\\Users\\Ignatios\\Documents\\ssd_keras\\keras_loss_function\\keras_ssd_loss.py:74: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From c:\\Users\\Ignatios\\Documents\\ssd_keras\\keras_loss_function\\keras_ssd_loss.py:166: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.cast` instead.\n"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = build_model(image_size=(img_height, img_width, img_channels),\n",
    "                    n_classes=n_classes,\n",
    "                    mode='training',\n",
    "                    l2_regularization=0.0007,\n",
    "                    scales=scales,\n",
    "                    aspect_ratios_global=aspect_ratios,\n",
    "                    aspect_ratios_per_layer=None,\n",
    "                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                    steps=steps,\n",
    "                    offsets=offsets,\n",
    "                    clip_boxes=clip_boxes,\n",
    "                    variances=variances,\n",
    "                    normalize_coords=normalize_coords,\n",
    "                    subtract_mean=intensity_mean,\n",
    "                    divide_by_stddev=intensity_range)\n",
    "\n",
    "adam = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the data generators for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loading labels: 100%|██████████| 614/614 [00:00<00:00, 2399.90it/s]\nLoading image IDs: 100%|██████████| 614/614 [00:00<00:00, 5077.57it/s]\nLoading labels: 100%|██████████| 77/77 [00:00<00:00, 2407.70it/s]\nLoading image IDs: 100%|██████████| 77/77 [00:00<00:00, 3502.15it/s]\nNumber of images in the training dataset:\t   614\nNumber of images in the validation dataset:\t    77\n"
    }
   ],
   "source": [
    "# Instantiate two `DataGenerator` objects: One for training, one for validation, loading the images into memory.\n",
    "\n",
    "train_dataset.create_hdf5_dataset(file_path='C:/Users/Ignatios/Documents/ssd_keras/hdf5_datasets/dataset_train.h5',\n",
    "                                  resize=False,\n",
    "                                  variable_image_size=True,\n",
    "                                  verbose=True)\n",
    "\n",
    "val_dataset.create_hdf5_dataset(file_path='C:/Users/Ignatios/Documents/ssd_keras/hdf5_datasets/dataset_val.h5',\n",
    "                                resize=False,\n",
    "                                variable_image_size=True,\n",
    "                                verbose=True)\n",
    "\n",
    "train_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path='C:/Users/Ignatios/Documents/ssd_keras/hdf5_datasets/dataset_train.h5')\n",
    "val_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path='C:/Users/Ignatios/Documents/ssd_keras/hdf5_datasets/dataset_val.h5')\n",
    "\n",
    "# Parse the image and label lists for the training and validation datasets.\n",
    "\n",
    "images_dir = 'C:/Users/Ignatios/Documents/ssd_keras/images/'\n",
    "\n",
    "train_labels_filename = 'C:/Users/Ignatios/Documents/ssd_keras/annotations/train_labels.csv'\n",
    "val_labels_filename   = 'C:/Users/Ignatios/Documents/ssd_keras/annotations/validation_labels.csv'\n",
    "\n",
    "train_dataset.parse_csv(images_dir=images_dir,\n",
    "                        labels_filename=train_labels_filename,\n",
    "                        input_format=['image_name', 'class_id', 'xmin', 'ymin', 'xmax', 'ymax'],\n",
    "                        include_classes='all')\n",
    "\n",
    "val_dataset.parse_csv(images_dir=images_dir,\n",
    "                      labels_filename=val_labels_filename,\n",
    "                      input_format=['image_name', 'class_id', 'xmin', 'ymin', 'xmax', 'ymax'],\n",
    "                      include_classes='all')\n",
    "\n",
    "train_dataset_size = train_dataset.get_dataset_size()\n",
    "val_dataset_size   = val_dataset.get_dataset_size()\n",
    "\n",
    "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
    "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data_generator.object_detection_2d_geometric_ops import Resize\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "\n",
    "# Set the batch size.\n",
    "batch_size = 16\n",
    "\n",
    "# For the validation generator:\n",
    "convert_to_3_channels = ConvertTo3Channels()\n",
    "resize = Resize(height=img_height, width=img_width)\n",
    "\n",
    "# Define the image processing chain.\n",
    "\n",
    "data_augmentation_chain = DataAugmentationConstantInputSize(random_brightness=(-48, 48, 0.5),\n",
    "                                                            random_contrast=(0.5, 1.8, 0.5),\n",
    "                                                            random_saturation=(0.5, 1.8, 0.5),\n",
    "                                                            random_hue=(18, 0.5),\n",
    "                                                            random_flip=0.5,\n",
    "                                                            random_translate=((0.03,0.5), (0.03,0.5), 0.5),\n",
    "                                                            random_scale=(0.5, 2.0, 0.5),\n",
    "                                                            n_trials_max=3,\n",
    "                                                            clip_boxes=True,\n",
    "                                                            overlap_criterion='area',\n",
    "                                                            bounds_box_filter=(0.3, 1.0),\n",
    "                                                            bounds_validator=(0.5, 1.0),\n",
    "                                                            n_boxes_min=1,\n",
    "                                                            background=(0,0,0))\n",
    "\n",
    "# Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
    "\n",
    "predictor_sizes = [model.get_layer('classes4').output_shape[1:3],\n",
    "                   model.get_layer('classes5').output_shape[1:3],\n",
    "                   model.get_layer('classes6').output_shape[1:3],\n",
    "                   model.get_layer('classes7').output_shape[1:3]]\n",
    "\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    scales=scales,\n",
    "                                    aspect_ratios_global=aspect_ratios,\n",
    "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                    steps=steps,\n",
    "                                    offsets=offsets,\n",
    "                                    clip_boxes=clip_boxes,\n",
    "                                    variances=variances,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.3,\n",
    "                                    normalize_coords=normalize_coords)\n",
    "\n",
    "# Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
    "\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[data_augmentation_chain, resize],\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "val_generator = val_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[resize],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the model callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(filepath='C:/Users/Ignatios/Documents/ssd_keras/model_weights/custom/ssd7_epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5',\n",
    "                                   monitor='val_loss',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=True,\n",
    "                                   mode='auto',\n",
    "                                   period=1)\n",
    "\n",
    "csv_logger = CSVLogger(filename='ssd7_training_log.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               min_delta=0.0,\n",
    "                               patience=6,\n",
    "                               verbose=1)\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                         factor=0.2,\n",
    "                                         patience=4,\n",
    "                                         verbose=1,\n",
    "                                         epsilon=0.001,\n",
    "                                         cooldown=0,\n",
    "                                         min_lr=0.00001)\n",
    "\n",
    "callbacks = [model_checkpoint,\n",
    "             csv_logger,\n",
    "             early_stopping,\n",
    "             reduce_learning_rate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "We force-stopped the training at 23 epochs to save time, since it started overfitting and did not improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow_gpu_object_detection\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:953: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n\nWARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow_gpu_object_detection\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:675: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nWARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow_gpu_object_detection\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:940: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n\nWARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow_gpu_object_detection\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2373: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n\nEpoch 1/25\nWARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow_gpu_object_detection\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:158: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n\nWARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow_gpu_object_detection\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:163: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n\nWARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow_gpu_object_detection\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:172: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nWARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow_gpu_object_detection\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n\nWARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow_gpu_object_detection\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:188: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n\n549/550 [============================>.] - ETA: 5s - loss: 5.9557Epoch 00001: val_loss improved from inf to 3.80833, saving model to C:/Users/Ignatios/Documents/ssd_keras/model_weights/custom/ssd7_epoch-01_loss-5.9568_val_loss-3.8083.h5\n550/550 [==============================] - 2781s 5s/step - loss: 5.9521 - val_loss: 3.8083\nEpoch 2/25\n549/550 [============================>.] - ETA: 5s - loss: 3.8676Epoch 00002: val_loss improved from 3.80833 to 3.30392, saving model to C:/Users/Ignatios/Documents/ssd_keras/model_weights/custom/ssd7_epoch-02_loss-3.8604_val_loss-3.3039.h5\n550/550 [==============================] - 2752s 5s/step - loss: 3.8667 - val_loss: 3.3039\nEpoch 3/25\n549/550 [============================>.] - ETA: 5s - loss: 3.4258Epoch 00003: val_loss improved from 3.30392 to 3.11663, saving model to C:/Users/Ignatios/Documents/ssd_keras/model_weights/custom/ssd7_epoch-03_loss-3.4218_val_loss-3.1166.h5\n550/550 [==============================] - 2756s 5s/step - loss: 3.4254 - val_loss: 3.1166\nEpoch 4/25\n549/550 [============================>.] - ETA: 5s - loss: 3.0825Epoch 00004: val_loss improved from 3.11663 to 3.00285, saving model to C:/Users/Ignatios/Documents/ssd_keras/model_weights/custom/ssd7_epoch-04_loss-3.0773_val_loss-3.0029.h5\n550/550 [==============================] - 2755s 5s/step - loss: 3.0824 - val_loss: 3.0029\nEpoch 5/25\n549/550 [============================>.] - ETA: 5s - loss: 2.7967Epoch 00005: val_loss improved from 3.00285 to 2.52359, saving model to C:/Users/Ignatios/Documents/ssd_keras/model_weights/custom/ssd7_epoch-05_loss-2.7933_val_loss-2.5236.h5\n550/550 [==============================] - 2770s 5s/step - loss: 2.7957 - val_loss: 2.5236\nEpoch 6/25\n549/550 [============================>.] - ETA: 4s - loss: 2.5823Epoch 00006: val_loss did not improve\n550/550 [==============================] - 2746s 5s/step - loss: 2.5814 - val_loss: 2.6245\nEpoch 7/25\n549/550 [============================>.] - ETA: 4s - loss: 2.4074Epoch 00007: val_loss improved from 2.52359 to 2.27540, saving model to C:/Users/Ignatios/Documents/ssd_keras/model_weights/custom/ssd7_epoch-07_loss-2.4070_val_loss-2.2754.h5\n550/550 [==============================] - 2747s 5s/step - loss: 2.4069 - val_loss: 2.2754\nEpoch 8/25\n549/550 [============================>.] - ETA: 4s - loss: 2.3061Epoch 00008: val_loss did not improve\n550/550 [==============================] - 2749s 5s/step - loss: 2.3053 - val_loss: 2.4112\nEpoch 9/25\n549/550 [============================>.] - ETA: 4s - loss: 2.1902Epoch 00009: val_loss improved from 2.27540 to 2.17172, saving model to C:/Users/Ignatios/Documents/ssd_keras/model_weights/custom/ssd7_epoch-09_loss-2.1898_val_loss-2.1717.h5\n550/550 [==============================] - 2728s 5s/step - loss: 2.1909 - val_loss: 2.1717\nEpoch 10/25\n549/550 [============================>.] - ETA: 5s - loss: 2.0953Epoch 00010: val_loss improved from 2.17172 to 2.12959, saving model to C:/Users/Ignatios/Documents/ssd_keras/model_weights/custom/ssd7_epoch-10_loss-2.0900_val_loss-2.1296.h5\n550/550 [==============================] - 2757s 5s/step - loss: 2.0965 - val_loss: 2.1296\nEpoch 11/25\n549/550 [============================>.] - ETA: 4s - loss: 2.0287Epoch 00011: val_loss did not improve\n550/550 [==============================] - 2754s 5s/step - loss: 2.0300 - val_loss: 2.4283\nEpoch 12/25\n549/550 [============================>.] - ETA: 5s - loss: 1.9616Epoch 00012: val_loss improved from 2.12959 to 2.09233, saving model to C:/Users/Ignatios/Documents/ssd_keras/model_weights/custom/ssd7_epoch-12_loss-1.9604_val_loss-2.0923.h5\n550/550 [==============================] - 2768s 5s/step - loss: 1.9608 - val_loss: 2.0923\nEpoch 13/25\n549/550 [============================>.] - ETA: 5s - loss: 1.8581Epoch 00013: val_loss did not improve\n550/550 [==============================] - 2753s 5s/step - loss: 1.8581 - val_loss: 2.1890\nEpoch 14/25\n549/550 [============================>.] - ETA: 4s - loss: 1.8420Epoch 00014: val_loss did not improve\n550/550 [==============================] - 2739s 5s/step - loss: 1.8426 - val_loss: 2.4376\nEpoch 15/25\n549/550 [============================>.] - ETA: 5s - loss: 1.7406Epoch 00015: val_loss did not improve\n550/550 [==============================] - 2768s 5s/step - loss: 1.7405 - val_loss: 2.1424\nEpoch 16/25\n549/550 [============================>.] - ETA: 4s - loss: 1.7233Epoch 00016: val_loss did not improve\n550/550 [==============================] - 2750s 5s/step - loss: 1.7237 - val_loss: 2.0933\nEpoch 17/25\n549/550 [============================>.] - ETA: 5s - loss: 1.6734Epoch 00017: val_loss did not improve\n\nEpoch 00017: reducing learning rate to 0.00010000000474974513.\n550/550 [==============================] - 2766s 5s/step - loss: 1.6735 - val_loss: 2.1189\nEpoch 18/25\n549/550 [============================>.] - ETA: 5s - loss: 1.4660Epoch 00018: val_loss improved from 2.09233 to 1.90817, saving model to C:/Users/Ignatios/Documents/ssd_keras/model_weights/custom/ssd7_epoch-18_loss-1.4620_val_loss-1.9082.h5\n550/550 [==============================] - 2788s 5s/step - loss: 1.4658 - val_loss: 1.9082\nEpoch 19/25\n549/550 [============================>.] - ETA: 5s - loss: 1.4058Epoch 00019: val_loss improved from 1.90817 to 1.87360, saving model to C:/Users/Ignatios/Documents/ssd_keras/model_weights/custom/ssd7_epoch-19_loss-1.4077_val_loss-1.8736.h5\n550/550 [==============================] - 2782s 5s/step - loss: 1.4050 - val_loss: 1.8736\nEpoch 20/25\n549/550 [============================>.] - ETA: 5s - loss: 1.4013Epoch 00020: val_loss did not improve\n550/550 [==============================] - 2793s 5s/step - loss: 1.4009 - val_loss: 1.9590\nEpoch 21/25\n549/550 [============================>.] - ETA: 5s - loss: 1.3631Epoch 00021: val_loss did not improve\n550/550 [==============================] - 2926s 5s/step - loss: 1.3630 - val_loss: 1.9394\nEpoch 22/25\n549/550 [============================>.] - ETA: 5s - loss: 1.3470Epoch 00022: val_loss did not improve\n550/550 [==============================] - 2865s 5s/step - loss: 1.3466 - val_loss: 1.9521\nEpoch 23/25\n 27/550 [>.............................] - ETA: 47:45 - loss: 1.3322"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-cbff46452102>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                               \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_dataset_size\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow_gpu_object_detection\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow_gpu_object_detection\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2063\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2064\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2065\u001b[1;33m                     \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2067\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow_gpu_object_detection\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    710\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m                     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_epoch   = 0\n",
    "final_epoch     = 25\n",
    "steps_per_epoch = 550\n",
    "\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=final_epoch,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=ceil(val_dataset_size/batch_size),\n",
    "                              initial_epoch=initial_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('tensorflow_gpu_object_detection': conda)",
   "language": "python",
   "name": "python37764bittensorflowgpuobjectdetectioncondabde55a80a8ba450d843c1fd84786c886"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}